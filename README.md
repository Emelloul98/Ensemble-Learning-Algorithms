# Ensemble Learning Algorithms: AdaBoost, K-Fold Cross Validation, and Bagging

A comprehensive comparison of ensemble learning algorithms including AdaBoost, K-Fold Cross Validation, and Bagging. This project focuses on evaluating their performance, accuracy, and computational efficiency. The analysis provides insight into the strengths and weaknesses of each algorithm across various datasets.

---

**Features:**
1. Implementation and comparison of AdaBoost, K-Fold Cross Validation, and Bagging.
2. Accuracy and performance testing across multiple datasets.
3. Visualizations demonstrating algorithm performance.
4. Analysis of bias-variance tradeoff using K-Fold validation.
5. Execution time comparison for different ensemble methods.

---

**Components:**
1. Python scripts implementing AdaBoost, K-Fold Cross Validation, and Bagging.
2. Jupyter Notebooks for data analysis, visualization, and evaluation.
3. CSV data files for testing different datasets and configurations.

---

**Running the Comparisons:**
1. Install Python and required libraries (scikit-learn, NumPy, pandas, matplotlib).
2. Run the Jupyter Notebook files to execute and visualize the comparisons.
3. Adjust dataset parameters and cross-validation folds for specific experiments.
4. Analyze the results, including accuracy, execution time, and bias-variance tradeoff.

---

**Example Visualizations:**

- Accuracy plots for AdaBoost, K-Fold, and Bagging across different datasets.
- Bias-variance evaluation using K-Fold Cross Validation.
- Execution time comparisons for ensemble algorithms on large datasets.

![AdaBoost Accuracy Plot](https://github.com/Emelloul98/Ensemble-Learning-Algorithms/blob/main/adaboost_plot.png)
![Bagging Performance Plot](https://github.com/Emelloul98/Ensemble-Learning-Algorithms/blob/main/bagging_plot.png)
